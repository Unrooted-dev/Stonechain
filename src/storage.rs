//! Stone Storage-Schicht
//!
//! Zwei unabhÃ¤ngige Speicher-Backends:
//!
//! ## 1. ChainStore (RocksDB)
//!
//! Persistiert die Blockchain in einer RocksDB-Datenbank mit 3 Column Families:
//!
//! | CF        | Key              | Value                          |
//! |-----------|------------------|--------------------------------|
//! | `blocks`  | index (8 Byte LE)| bincode-serialisierter Block   |
//! | `meta`    | UTF-8-String     | UTF-8-Wert                     |
//! | `index`   | doc_id (UTF-8)   | block_index (8 Byte LE)        |
//!
//! Meta-EintrÃ¤ge:
//!   - `"latest_hash"`    â†’ aktueller Chain-Hash
//!   - `"block_count"`    â†’ Anzahl BlÃ¶cke (8 Byte LE)
//!   - `"genesis_hash"`   â†’ Hash des Genesis-Blocks
//!
//! ## 2. ChunkStore (Lokales Dateisystem)
//!
//! Speichert Dokument-Chunks als einzelne Dateien unter `stone_data/chunks/<sha256-hex>`.
//! - Inhaltsadressiert: Dateiname = SHA-256 des Inhalts
//! - Deduplizierung automatisch (gleiche Bytes â†’ gleicher Hash â†’ eine Datei)
//! - Lesen, Schreiben, Existenz-Check, GrÃ¶ÃŸe, AufrÃ¤umen (Garbage Collection)

use crate::blockchain::{Block, Document, chunk_dir, data_dir, ChunkRef, CHUNK_SIZE as BLOCK_CHUNK_SIZE};
use crate::shard::{self, ShardStore};
use bincode::config::standard;
use rocksdb::{ColumnFamilyDescriptor, DB, Options, WriteBatch, WriteOptions};
use sha2::{Digest, Sha256};
use std::path::PathBuf;
use std::sync::Arc;

// â”€â”€â”€ Pfade â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

pub fn chain_db_path() -> String { format!("{}/chain_db", data_dir()) }

// â”€â”€â”€ Fehler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#[derive(Debug)]
pub enum StorageError {
    Rocks(rocksdb::Error),
    Encode(String),
    Decode(String),
    Io(std::io::Error),
    NotFound(String),
}

impl std::fmt::Display for StorageError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Rocks(e) => write!(f, "RocksDB: {e}"),
            Self::Encode(s) => write!(f, "Kodierung: {s}"),
            Self::Decode(s) => write!(f, "Dekodierung: {s}"),
            Self::Io(e) => write!(f, "IO: {e}"),
            Self::NotFound(s) => write!(f, "Nicht gefunden: {s}"),
        }
    }
}

impl From<rocksdb::Error> for StorageError {
    fn from(e: rocksdb::Error) -> Self { Self::Rocks(e) }
}

impl From<std::io::Error> for StorageError {
    fn from(e: std::io::Error) -> Self { Self::Io(e) }
}

// â”€â”€â”€ ChainStore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// RocksDB-basierter Chain-Speicher.
///
/// Wird als `Arc<ChainStore>` geteilt â€“ RocksDB selbst ist thread-safe.
pub struct ChainStore {
    db: Arc<DB>,
}

impl ChainStore {
    /// Ã–ffnet (oder erstellt) die RocksDB-Datenbank.
    pub fn open() -> Result<Arc<Self>, StorageError> {
        std::fs::create_dir_all(data_dir())?;

        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.create_missing_column_families(true);
        opts.set_compression_type(rocksdb::DBCompressionType::Snappy);

        // Column Families definieren
        let cf_blocks = ColumnFamilyDescriptor::new("blocks", Options::default());
        let cf_meta   = ColumnFamilyDescriptor::new("meta",   Options::default());
        let cf_index  = ColumnFamilyDescriptor::new("index",  Options::default());

        let db = DB::open_cf_descriptors(
            &opts,
            chain_db_path(),
            vec![cf_blocks, cf_meta, cf_index],
        )?;

        Ok(Arc::new(Self { db: Arc::new(db) }))
    }

    // â”€â”€â”€ Schreiben â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Schreibt einen Block atomar in RocksDB.
    ///
    /// - `blocks` CF:  index_le â†’ bincode(Block)
    /// - `index`  CF:  doc_id â†’ block_index_le  (fÃ¼r jeden doc_id im Block)
    /// - `meta`   CF:  "latest_hash" + "block_count" aktualisieren
    pub fn write_block(&self, block: &Block) -> Result<(), StorageError> {
        let cf_blocks = self.db.cf_handle("blocks")
            .ok_or_else(|| StorageError::NotFound("CF 'blocks' nicht gefunden".into()))?;
        let cf_meta = self.db.cf_handle("meta")
            .ok_or_else(|| StorageError::NotFound("CF 'meta' nicht gefunden".into()))?;
        let cf_index = self.db.cf_handle("index")
            .ok_or_else(|| StorageError::NotFound("CF 'index' nicht gefunden".into()))?;

        // Block bincode-serialisieren
        let encoded = bincode::serde::encode_to_vec(block, standard())
            .map_err(|e| StorageError::Encode(e.to_string()))?;

        let key_index = block.index.to_le_bytes();
        let block_index_bytes = block.index.to_le_bytes();

        let mut batch = WriteBatch::default();

        // Block speichern
        batch.put_cf(cf_blocks, key_index, &encoded);

        // Dokument-Index aktualisieren
        for doc in &block.documents {
            batch.put_cf(cf_index, doc.doc_id.as_bytes(), block_index_bytes);
        }

        // Metadaten
        batch.put_cf(cf_meta, b"latest_hash", block.hash.as_bytes());
        let count = (block.index + 1).to_le_bytes();
        batch.put_cf(cf_meta, b"block_count", count);

        // Genesis-Hash beim ersten Block merken
        if block.index == 0 {
            batch.put_cf(cf_meta, b"genesis_hash", block.hash.as_bytes());
        }

        self.db.write(batch)?;
        Ok(())
    }

    /// Schreibt einen Block mit sofortigem WAL-Sync (fÃ¼r kritische Persistenz).
    ///
    /// Identisch zu `write_block`, aber mit `sync = true` in den WriteOptions,
    /// damit der Block nach dem Aufruf garantiert auf Disk ist.
    pub fn write_block_sync(&self, block: &Block) -> Result<(), StorageError> {
        let cf_blocks = self.db.cf_handle("blocks")
            .ok_or_else(|| StorageError::NotFound("CF 'blocks' nicht gefunden".into()))?;
        let cf_meta = self.db.cf_handle("meta")
            .ok_or_else(|| StorageError::NotFound("CF 'meta' nicht gefunden".into()))?;
        let cf_index = self.db.cf_handle("index")
            .ok_or_else(|| StorageError::NotFound("CF 'index' nicht gefunden".into()))?;

        let encoded = bincode::serde::encode_to_vec(block, standard())
            .map_err(|e| StorageError::Encode(e.to_string()))?;

        let key_index = block.index.to_le_bytes();
        let block_index_bytes = block.index.to_le_bytes();

        let mut batch = WriteBatch::default();
        batch.put_cf(cf_blocks, key_index, &encoded);
        for doc in &block.documents {
            batch.put_cf(cf_index, doc.doc_id.as_bytes(), block_index_bytes);
        }
        batch.put_cf(cf_meta, b"latest_hash", block.hash.as_bytes());
        let count = (block.index + 1).to_le_bytes();
        batch.put_cf(cf_meta, b"block_count", count);
        if block.index == 0 {
            batch.put_cf(cf_meta, b"genesis_hash", block.hash.as_bytes());
        }

        // WAL sofort auf Disk flushen â†’ Ã¼berlebt Absturz / abruptes Beenden
        let mut wo = WriteOptions::default();
        wo.set_sync(true);
        self.db.write_opt(batch, &wo)?;
        Ok(())
    }

    // â”€â”€â”€ Lesen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Liest einen einzelnen Block anhand seines Index.
    pub fn read_block(&self, index: u64) -> Result<Block, StorageError> {
        let cf = self.db.cf_handle("blocks")
            .ok_or_else(|| StorageError::NotFound("CF 'blocks'".into()))?;
        let key = index.to_le_bytes();
        let data = self.db.get_cf(cf, key)?
            .ok_or_else(|| StorageError::NotFound(format!("Block #{index}")))?;
        let (block, _) = bincode::serde::decode_from_slice::<Block, _>(&data, standard())
            .map_err(|e| StorageError::Decode(e.to_string()))?;
        Ok(block)
    }

    /// Liest alle BlÃ¶cke in Reihenfolge (0 â†’ n).
    pub fn read_all_blocks(&self) -> Result<Vec<Block>, StorageError> {
        let cf = self.db.cf_handle("blocks")
            .ok_or_else(|| StorageError::NotFound("CF 'blocks'".into()))?;

        let count = self.block_count()?;
        let mut blocks = Vec::with_capacity(count as usize);

        for i in 0..count {
            let key = i.to_le_bytes();
            if let Some(data) = self.db.get_cf(cf, key)? {
                let (block, _) = bincode::serde::decode_from_slice::<Block, _>(&data, standard())
                    .map_err(|e| StorageError::Decode(e.to_string()))?;
                blocks.push(block);
            }
        }
        Ok(blocks)
    }

    /// Liest eine Range von BlÃ¶cken (von `from` bis `to` exklusiv).
    pub fn read_blocks_range(&self, from: u64, to: u64) -> Result<Vec<Block>, StorageError> {
        let cf = self.db.cf_handle("blocks")
            .ok_or_else(|| StorageError::NotFound("CF 'blocks'".into()))?;
        let mut blocks = Vec::new();
        for i in from..to {
            let key = i.to_le_bytes();
            if let Some(data) = self.db.get_cf(cf, key)? {
                let (block, _) = bincode::serde::decode_from_slice::<Block, _>(&data, standard())
                    .map_err(|e| StorageError::Decode(e.to_string()))?;
                blocks.push(block);
            }
        }
        Ok(blocks)
    }

    // â”€â”€â”€ Dokument-Abfragen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Gibt den Block-Index zurÃ¼ck in dem `doc_id` zuletzt gespeichert wurde.
    pub fn find_block_for_doc(&self, doc_id: &str) -> Result<Option<u64>, StorageError> {
        let cf = self.db.cf_handle("index")
            .ok_or_else(|| StorageError::NotFound("CF 'index'".into()))?;
        match self.db.get_cf(cf, doc_id.as_bytes())? {
            Some(bytes) if bytes.len() == 8 => {
                let arr: [u8; 8] = bytes.try_into().unwrap();
                Ok(Some(u64::from_le_bytes(arr)))
            }
            _ => Ok(None),
        }
    }

    /// Gibt den Block direkt zurÃ¼ck der `doc_id` enthÃ¤lt (neueste Version).
    pub fn find_block_containing_doc(&self, doc_id: &str) -> Result<Option<Block>, StorageError> {
        if let Some(idx) = self.find_block_for_doc(doc_id)? {
            Ok(Some(self.read_block(idx)?))
        } else {
            Ok(None)
        }
    }

    // â”€â”€â”€ Metadaten â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Aktueller Latest-Hash der Chain.
    pub fn latest_hash(&self) -> Result<String, StorageError> {
        let cf = self.db.cf_handle("meta")
            .ok_or_else(|| StorageError::NotFound("CF 'meta'".into()))?;
        match self.db.get_cf(cf, b"latest_hash")? {
            Some(bytes) => Ok(String::from_utf8_lossy(&bytes).to_string()),
            None => Ok(String::new()),
        }
    }

    /// Anzahl gespeicherter BlÃ¶cke.
    pub fn block_count(&self) -> Result<u64, StorageError> {
        let cf = self.db.cf_handle("meta")
            .ok_or_else(|| StorageError::NotFound("CF 'meta'".into()))?;
        match self.db.get_cf(cf, b"block_count")? {
            Some(bytes) if bytes.len() == 8 => {
                let arr: [u8; 8] = bytes.try_into().unwrap();
                Ok(u64::from_le_bytes(arr))
            }
            _ => Ok(0),
        }
    }

    /// Genesis-Hash (leer = keine Chain vorhanden).
    pub fn genesis_hash(&self) -> Result<String, StorageError> {
        let cf = self.db.cf_handle("meta")
            .ok_or_else(|| StorageError::NotFound("CF 'meta'".into()))?;
        match self.db.get_cf(cf, b"genesis_hash")? {
            Some(bytes) => Ok(String::from_utf8_lossy(&bytes).to_string()),
            None => Ok(String::new()),
        }
    }

    /// Gibt true zurÃ¼ck wenn die Datenbank leer ist (kein Genesis-Block).
    pub fn is_empty(&self) -> bool {
        self.block_count().unwrap_or(0) == 0
    }

    // â”€â”€â”€ Diagnose / Admin â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Gibt eine Zusammenfassung der gespeicherten Daten zurÃ¼ck.
    pub fn summary(&self) -> StoreSummary {
        let block_count = self.block_count().unwrap_or(0);
        let latest_hash = self.latest_hash().unwrap_or_default();
        let genesis_hash = self.genesis_hash().unwrap_or_default();
        StoreSummary { block_count, latest_hash, genesis_hash }
    }
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct StoreSummary {
    pub block_count: u64,
    pub latest_hash: String,
    pub genesis_hash: String,
}

// â”€â”€â”€ ChunkStore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Lokaler inhaltsadressierter Speicher fÃ¼r Dokument-Chunks.
///
/// Layout: `stone_data/chunks/<sha256-hex-64>` (eine Datei pro Chunk)
///
/// Vorteile:
/// - Automatische Deduplizierung (gleicher Inhalt = gleiche Datei)
/// - Kein Datenbankoverhead fÃ¼r BinÃ¤rdaten
/// - Einfaches Backup: Verzeichnis kopieren
/// - Peer-Sync: fehlt eine Datei â†’ bei Peer abrufen
#[derive(Clone)]
pub struct ChunkStore {
    base_dir: PathBuf,
}

impl ChunkStore {
    /// Erstellt einen ChunkStore (legt Verzeichnis an falls nÃ¶tig).
    pub fn new() -> Result<Self, StorageError> {
        let base_dir = PathBuf::from(chunk_dir());
        std::fs::create_dir_all(&base_dir)?;
        Ok(Self { base_dir })
    }

    /// Pfad zu einem Chunk anhand seines Hashes.
    fn chunk_path(&self, hash: &str) -> PathBuf {
        self.base_dir.join(hash)
    }

    // â”€â”€â”€ Schreiben â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Schreibt einen Chunk und gibt seinen SHA-256-Hash zurÃ¼ck.
    ///
    /// Wenn der Chunk bereits existiert wird er nicht Ã¼berschrieben (Deduplizierung).
    pub fn write_chunk(&self, data: &[u8]) -> Result<String, StorageError> {
        let hash = self.compute_hash(data);
        let path = self.chunk_path(&hash);
        if !path.exists() {
            std::fs::write(&path, data)?;
        }
        Ok(hash)
    }

    /// Schreibt mehrere Chunks auf einmal und gibt ihre Hashes zurÃ¼ck.
    pub fn write_chunks(&self, data: &[u8], chunk_size: usize) -> Result<Vec<crate::blockchain::ChunkRef>, StorageError> {
        let mut refs = Vec::new();
        for chunk in data.chunks(chunk_size.max(1)) {
            let hash = self.write_chunk(chunk)?;
            refs.push(crate::blockchain::ChunkRef {
                hash,
                size: chunk.len() as u64,
                shards: Vec::new(),
                ec_k: 0,
                ec_m: 0,
            });
        }
        Ok(refs)
    }

    // â”€â”€â”€ Lesen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Liest einen einzelnen Chunk anhand seines Hashes.
    pub fn read_chunk(&self, hash: &str) -> Result<Vec<u8>, StorageError> {
        let path = self.chunk_path(hash);
        std::fs::read(&path).map_err(|_| {
            StorageError::NotFound(format!("Chunk {hash} nicht gefunden"))
        })
    }

    /// Rekonstruiert ein Dokument aus seinen Chunks.
    pub fn reconstruct_document(&self, doc: &Document) -> Result<Vec<u8>, StorageError> {
        if doc.chunks.is_empty() {
            return Err(StorageError::NotFound(format!(
                "Dokument '{}' hat keine Chunks", doc.doc_id
            )));
        }
        let mut data = Vec::new();
        for ch in &doc.chunks {
            let bytes = self.read_chunk(&ch.hash)?;
            if bytes.len() as u64 != ch.size {
                return Err(StorageError::Decode(format!(
                    "Chunk {}: GrÃ¶ÃŸe stimmt nicht ({} != {})", ch.hash, bytes.len(), ch.size
                )));
            }
            data.extend_from_slice(&bytes);
        }
        Ok(data)
    }

    // â”€â”€â”€ Abfragen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Gibt true zurÃ¼ck wenn ein Chunk mit diesem Hash vorhanden ist.
    pub fn has_chunk(&self, hash: &str) -> bool {
        self.chunk_path(hash).exists()
    }

    /// Gibt die GrÃ¶ÃŸe eines Chunks zurÃ¼ck (None falls nicht vorhanden).
    pub fn chunk_size(&self, hash: &str) -> Option<u64> {
        std::fs::metadata(self.chunk_path(hash)).ok().map(|m| m.len())
    }

    /// Gibt alle gespeicherten Chunk-Hashes zurÃ¼ck.
    pub fn list_chunks(&self) -> Vec<String> {
        std::fs::read_dir(&self.base_dir)
            .map(|entries| {
                entries
                    .flatten()
                    .filter_map(|e| {
                        let name = e.file_name().to_string_lossy().to_string();
                        if name.len() == 64 && name.chars().all(|c| c.is_ascii_hexdigit()) {
                            Some(name)
                        } else {
                            None
                        }
                    })
                    .collect()
            })
            .unwrap_or_default()
    }

    /// GesamtgrÃ¶ÃŸe aller gespeicherten Chunks in Bytes.
    pub fn total_size_bytes(&self) -> u64 {
        self.list_chunks()
            .iter()
            .filter_map(|h| self.chunk_size(h))
            .sum()
    }

    /// Anzahl gespeicherter Chunks.
    pub fn chunk_count(&self) -> usize {
        self.list_chunks().len()
    }

    // â”€â”€â”€ Garbage Collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    /// Entfernt alle Chunks die von keinem Block mehr referenziert werden.
    ///
    /// Gibt zurÃ¼ck wie viele Chunks gelÃ¶scht wurden und wie viele Bytes freigegeben.
    pub fn gc(&self, referenced_hashes: &std::collections::HashSet<String>) -> GcResult {
        let all = self.list_chunks();
        let mut deleted = 0usize;
        let mut freed_bytes = 0u64;

        for hash in &all {
            if !referenced_hashes.contains(hash.as_str()) {
                if let Some(size) = self.chunk_size(hash) {
                    let _ = std::fs::remove_file(self.chunk_path(hash));
                    deleted += 1;
                    freed_bytes += size;
                }
            }
        }

        GcResult { deleted, freed_bytes }
    }

    // â”€â”€â”€ Intern â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    fn compute_hash(&self, data: &[u8]) -> String {
        let mut h = Sha256::new();
        h.update(data);
        hex::encode(h.finalize())
    }
}

impl Default for ChunkStore {
    fn default() -> Self {
        Self::new().expect("ChunkStore konnte nicht erstellt werden")
    }
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct GcResult {
    pub deleted: usize,
    pub freed_bytes: u64,
}

// â”€â”€â”€ Kombinierter Store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Fasst ChainStore, ChunkStore und ShardStore zusammen.
/// Wird als `Arc<StoneStore>` in der gesamten Anwendung geteilt.
pub struct StoneStore {
    pub chain: Arc<ChainStore>,
    pub chunks: ChunkStore,
    pub shards: ShardStore,
}

impl StoneStore {
    pub fn open() -> Result<Arc<Self>, StorageError> {
        let chain = ChainStore::open()?;
        let chunks = ChunkStore::new()?;
        let shards = ShardStore::new().map_err(|e| StorageError::Io(
            std::io::Error::new(std::io::ErrorKind::Other, e.to_string())
        ))?;
        Ok(Arc::new(Self { chain, chunks, shards }))
    }

    /// Sammelt alle von der aktuellen Chain referenzierten Chunk-Hashes.
    pub fn referenced_chunks(&self) -> Result<std::collections::HashSet<String>, StorageError> {
        let blocks = self.chain.read_all_blocks()?;
        let mut hashes = std::collections::HashSet::new();
        for block in &blocks {
            for doc in &block.documents {
                for ch in &doc.chunks {
                    hashes.insert(ch.hash.clone());
                }
            }
        }
        Ok(hashes)
    }

    /// Garbage Collection: entfernt alle nicht mehr referenzierten Chunks.
    pub fn gc(&self) -> Result<GcResult, StorageError> {
        let referenced = self.referenced_chunks()?;
        Ok(self.chunks.gc(&referenced))
    }

    /// Liest Dokument-Daten â€” automatisch aus Chunks (Legacy) oder Shards (Erasure-Coded).
    pub fn read_document_data(&self, doc: &Document) -> Result<Vec<u8>, StorageError> {
        let mut data = Vec::new();
        for chunk_ref in &doc.chunks {
            let chunk_data = if chunk_ref.shards.is_empty() {
                // Legacy: Full-Replication Chunk
                self.chunks.read_chunk(&chunk_ref.hash)?
            } else {
                // Neu: Erasure-Coded Shards â€“ lokale Rekonstruktion versuchen
                self.shards
                    .try_reconstruct_local(
                        &chunk_ref.hash,
                        chunk_ref.ec_k,
                        chunk_ref.ec_m,
                        chunk_ref.size as usize,
                    )
                    .map_err(|e| StorageError::NotFound(e.to_string()))?
            };
            data.extend_from_slice(&chunk_data);
        }
        Ok(data)
    }

    /// Erasure-Coded ein Dokument: Nimmt die Roh-Bytes und bestehende ChunkRefs,
    /// encoded jeden Chunk mit Reed-Solomon, speichert alle Shards lokal,
    /// und gibt aktualisierte ChunkRefs mit ShardRef-Infos zurÃ¼ck.
    ///
    /// Der `local_peer_id` wird als `holder` fÃ¼r die lokal gespeicherten Shards gesetzt.
    ///
    /// Parameter:
    /// - `raw_data`: Die vollstÃ¤ndigen Dokument-Bytes (vor oder nach VerschlÃ¼sselung)
    /// - `chunk_refs`: Bestehende ChunkRefs (aus `write_chunks()`)
    /// - `local_peer_id`: PeerId dieser Node (als Holder-Referenz)
    /// - `k`: Anzahl Daten-Shards (Default: 4)
    /// - `m`: Anzahl Parity-Shards (Default: 2)
    pub fn erasure_code_chunks(
        &self,
        raw_data: &[u8],
        chunk_refs: &[ChunkRef],
        local_peer_id: &str,
        k: u8,
        m: u8,
    ) -> Result<Vec<ChunkRef>, StorageError> {
        let chunk_size = BLOCK_CHUNK_SIZE;
        let mut coded_refs = Vec::with_capacity(chunk_refs.len());

        for (i, chunk_ref) in chunk_refs.iter().enumerate() {
            let start = i * chunk_size;
            let end = (start + chunk_size).min(raw_data.len());
            let chunk_data = &raw_data[start..end];

            // Reed-Solomon Encoding
            let shard_pieces = shard::encode_chunk(chunk_data, k as usize, m as usize)
                .map_err(|e| StorageError::Encode(format!("Erasure-Coding Chunk {}: {e}", chunk_ref.hash)))?;

            // Alle Shards lokal speichern (diese Node hÃ¤lt erstmal alle)
            let shard_tuples: Vec<(u8, Vec<u8>)> = shard_pieces
                .into_iter()
                .enumerate()
                .map(|(idx, data)| (idx as u8, data))
                .collect();

            let mut shard_refs = self.shards
                .write_my_shards(&chunk_ref.hash, &shard_tuples)
                .map_err(|e| StorageError::Io(
                    std::io::Error::new(std::io::ErrorKind::Other, e.to_string())
                ))?;

            // Holder auf local_peer_id setzen
            for sr in &mut shard_refs {
                sr.holder = local_peer_id.to_string();
            }

            coded_refs.push(ChunkRef {
                hash: chunk_ref.hash.clone(),
                size: chunk_ref.size,
                shards: shard_refs,
                ec_k: k,
                ec_m: m,
            });
        }

        Ok(coded_refs)
    }
}

// â”€â”€â”€ Shard-Verteilung (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Verteilt Shards eines Dokuments an verbundene Peers via P2P ShardExchange.
///
/// Strategie:
/// - Holt die Liste der verbundenen Peers
/// - Weist Shards per Round-Robin an Peers zu (assign_shards_to_peers)
/// - Sendet jeden zugewiesenen Shard via `store_shard_on_peer()`
/// - Wartet NICHT auf BestÃ¤tigung (fire-and-forget; BestÃ¤tigungen kommen als NetworkEvent)
///
/// Gibt die aktualisierten ChunkRefs zurÃ¼ck mit den Holder-Infos.
pub async fn distribute_shards(
    chunk_refs: &[ChunkRef],
    shard_store: &ShardStore,
    network: &crate::network::NetworkHandle,
) -> Vec<ChunkRef> {
    // Verbundene Peers holen
    let peers = network.connected_peers().await;
    if peers.is_empty() {
        println!("[sharding] âš  Keine verbundenen Peers â€“ Shards bleiben nur lokal");
        return chunk_refs.to_vec();
    }

    let peer_ids: Vec<String> = peers.iter().map(|p| p.peer_id.clone()).collect();
    println!(
        "[sharding] ğŸ“¤ Verteile Shards an {} Peer(s): {:?}",
        peer_ids.len(),
        peer_ids.iter().map(|p| &p[..8.min(p.len())]).collect::<Vec<_>>()
    );

    let mut updated_refs = Vec::with_capacity(chunk_refs.len());

    for chunk_ref in chunk_refs {
        if chunk_ref.shards.is_empty() || chunk_ref.ec_k == 0 {
            // Nicht erasure-coded â†’ Ã¼berspringen
            updated_refs.push(chunk_ref.clone());
            continue;
        }

        let k = chunk_ref.ec_k;
        let m = chunk_ref.ec_m;

        // Shard-Zuweisung berechnen
        let assignments = shard::assign_shards_to_peers(
            &chunk_ref.hash,
            &peer_ids,
            k,
            m,
        );

        let mut updated_shards = chunk_ref.shards.clone();

        for (shard_index, target_peer_id) in &assignments {
            // Shard lokal lesen
            let shard_data = match shard_store.read_shard(&chunk_ref.hash, *shard_index) {
                Ok(data) => data,
                Err(e) => {
                    eprintln!(
                        "[sharding] âŒ Kann Shard {}[{}] nicht lesen: {e}",
                        &chunk_ref.hash[..8], shard_index
                    );
                    continue;
                }
            };

            // Shard-Hash fÃ¼r IntegritÃ¤t
            let hash = shard::shard_hash(&shard_data);

            // PeerId parsen
            let peer_id: libp2p::PeerId = match target_peer_id.parse() {
                Ok(id) => id,
                Err(e) => {
                    eprintln!("[sharding] âŒ UngÃ¼ltige PeerId {target_peer_id}: {e}");
                    continue;
                }
            };

            println!(
                "[sharding] â†’ Shard {}[{}] ({} bytes) â†’ {}",
                &chunk_ref.hash[..8.min(chunk_ref.hash.len())],
                shard_index,
                shard_data.len(),
                &target_peer_id[..8.min(target_peer_id.len())]
            );

            // An Peer senden (fire-and-forget)
            network
                .store_shard_on_peer(
                    peer_id,
                    chunk_ref.hash.clone(),
                    *shard_index,
                    hash,
                    shard_data,
                )
                .await;

            // Holder in den ShardRefs aktualisieren
            if let Some(sr) = updated_shards
                .iter_mut()
                .find(|s| s.shard_index == *shard_index)
            {
                // ZusÃ¤tzlichen Holder vermerken (Komma-getrennt)
                if !sr.holder.is_empty() {
                    sr.holder = format!("{},{}", sr.holder, target_peer_id);
                } else {
                    sr.holder = target_peer_id.clone();
                }
            }
        }

        updated_refs.push(ChunkRef {
            hash: chunk_ref.hash.clone(),
            size: chunk_ref.size,
            shards: updated_shards,
            ec_k: k,
            ec_m: m,
        });
    }

    println!(
        "[sharding] âœ… Verteilung fÃ¼r {} Chunk(s) abgeschlossen",
        updated_refs.len()
    );
    updated_refs
}

// â”€â”€â”€ Shard-Download (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Holt fehlende Shards von Peers und rekonstruiert ein Dokument.
///
/// Ablauf fÃ¼r jeden Chunk eines Dokuments:
/// 1. Versucht lokale Rekonstruktion (try_reconstruct_local)
/// 2. Bei Fehler: PrÃ¼ft welche Shards lokal fehlen
/// 3. Holt fehlende Shards von Peers (anhand der Holder-Infos in ShardRef)
/// 4. Rekonstruiert mit reconstruct_with_remote()
///
/// Gibt die vollstÃ¤ndigen Dokument-Bytes zurÃ¼ck.
pub async fn read_document_with_remote_shards(
    doc: &Document,
    shard_store: &ShardStore,
    chunk_store: &ChunkStore,
    network: &crate::network::NetworkHandle,
) -> Result<Vec<u8>, StorageError> {
    let mut data = Vec::new();

    for chunk_ref in &doc.chunks {
        let chunk_data = if chunk_ref.shards.is_empty() {
            // Legacy: kein Erasure Coding â†’ direkt aus Chunk-Store
            chunk_store.read_chunk(&chunk_ref.hash)?
        } else {
            // Erasure-Coded: Erst lokal versuchen
            match shard_store.try_reconstruct_local(
                &chunk_ref.hash,
                chunk_ref.ec_k,
                chunk_ref.ec_m,
                chunk_ref.size as usize,
            ) {
                Ok(bytes) => bytes,
                Err(local_err) => {
                    // Lokale Rekonstruktion fehlgeschlagen â†’ Remote-Shards holen
                    println!(
                        "[sharding] â¬‡ï¸ Lokale Rekonstruktion fehlgeschlagen fÃ¼r {}: {local_err}",
                        &chunk_ref.hash[..8.min(chunk_ref.hash.len())]
                    );

                    let remote_shards = fetch_missing_shards_for_chunk(
                        chunk_ref,
                        shard_store,
                        network,
                    ).await;

                    if remote_shards.is_empty() {
                        return Err(StorageError::NotFound(format!(
                            "Chunk {} nicht rekonstruierbar: keine Remote-Shards verfÃ¼gbar",
                            chunk_ref.hash
                        )));
                    }

                    // Jetzt mit lokalen + remote Shards rekonstruieren
                    shard_store
                        .reconstruct_with_remote(
                            &chunk_ref.hash,
                            &remote_shards,
                            chunk_ref.ec_k,
                            chunk_ref.ec_m,
                            chunk_ref.size as usize,
                        )
                        .map_err(|e| StorageError::NotFound(format!(
                            "Rekonstruktion fehlgeschlagen fÃ¼r Chunk {}: {e}",
                            chunk_ref.hash
                        )))?
                }
            }
        };
        data.extend_from_slice(&chunk_data);
    }

    Ok(data)
}

/// Holt fehlende Shards fÃ¼r einen bestimmten Chunk von Peers.
///
/// Strategie:
/// 1. Ermittelt welche Shard-Indices lokal fehlen
/// 2. FÃ¼r jeden fehlenden Shard: prÃ¼ft die Holder-Liste in ShardRef
/// 3. Fordert Shard von dem/den Holder-Peer(s) an
/// 4. Wartet auf Antwort (mit Timeout)
///
/// Gibt eine Map shard_index â†’ shard_data zurÃ¼ck.
async fn fetch_missing_shards_for_chunk(
    chunk_ref: &ChunkRef,
    shard_store: &ShardStore,
    network: &crate::network::NetworkHandle,
) -> std::collections::HashMap<u8, Vec<u8>> {
    use std::collections::HashMap;
    use tokio::sync::broadcast;

    let local_indices = shard_store.local_shard_indices(&chunk_ref.hash);
    let k = chunk_ref.ec_k as usize;

    // Wie viele Shards brauchen wir noch?
    let needed = if local_indices.len() >= k {
        return HashMap::new(); // Genug lokal!
    } else {
        k - local_indices.len()
    };

    println!(
        "[sharding] ğŸ” Brauche {} weitere Shard(s) fÃ¼r {} (lokal: {:?})",
        needed,
        &chunk_ref.hash[..8.min(chunk_ref.hash.len())],
        local_indices
    );

    // Fehlende Shard-Indices bestimmen
    let total_shards = (chunk_ref.ec_k + chunk_ref.ec_m) as u8;
    let missing_indices: Vec<u8> = (0..total_shards)
        .filter(|i| !local_indices.contains(i))
        .collect();

    // Event-Listener fÃ¼r eingehende Shards
    let mut event_rx = network.subscribe();
    let mut fetched: HashMap<u8, Vec<u8>> = HashMap::new();

    // Shard-Anfragen senden
    for shard_ref in &chunk_ref.shards {
        if !missing_indices.contains(&shard_ref.shard_index) {
            continue; // Haben wir schon lokal
        }

        // Holder kann kommagetrennt mehrere PeerIds enthalten
        let holders: Vec<&str> = shard_ref.holder.split(',').collect();
        for holder_id in holders {
            let holder_id = holder_id.trim();
            if holder_id.is_empty() || holder_id == network.local_peer_id {
                continue;
            }

            if let Ok(peer_id) = holder_id.parse::<libp2p::PeerId>() {
                println!(
                    "[sharding] â†’ Anfrage Shard {}[{}] von {}",
                    &chunk_ref.hash[..8.min(chunk_ref.hash.len())],
                    shard_ref.shard_index,
                    &holder_id[..8.min(holder_id.len())]
                );
                network
                    .request_shard(peer_id, chunk_ref.hash.clone(), shard_ref.shard_index)
                    .await;
                break; // Nur von einem Holder anfordern (retry bei nÃ¤chstem Versuch)
            }
        }
    }

    // Fallback: Wenn keine Holder bekannt sind, alle verbundenen Peers fragen
    if chunk_ref.shards.iter().all(|s| s.holder.is_empty()) {
        let peers = network.connected_peers().await;
        for missing_idx in &missing_indices {
            if fetched.len() >= needed {
                break;
            }
            for peer in &peers {
                if let Ok(peer_id) = peer.peer_id.parse::<libp2p::PeerId>() {
                    println!(
                        "[sharding] â†’ Broadcast-Anfrage Shard {}[{}] an {}",
                        &chunk_ref.hash[..8.min(chunk_ref.hash.len())],
                        missing_idx,
                        &peer.peer_id[..8.min(peer.peer_id.len())]
                    );
                    network
                        .request_shard(peer_id, chunk_ref.hash.clone(), *missing_idx)
                        .await;
                }
            }
        }
    }

    // Auf Antworten warten (Timeout: 10 Sekunden)
    let deadline = tokio::time::Instant::now() + tokio::time::Duration::from_secs(10);

    loop {
        if fetched.len() >= needed {
            break;
        }

        let remaining = deadline - tokio::time::Instant::now();
        if remaining.is_zero() {
            eprintln!(
                "[sharding] â° Timeout: {} von {} benÃ¶tigten Shards empfangen",
                fetched.len(),
                needed
            );
            break;
        }

        match tokio::time::timeout(remaining, event_rx.recv()).await {
            Ok(Ok(crate::network::NetworkEvent::ShardReceived {
                chunk_hash,
                shard_index,
                data,
                from_peer,
            })) if chunk_hash == chunk_ref.hash => {
                println!(
                    "[sharding] â† Shard {}[{}] empfangen von {} ({} bytes)",
                    &chunk_hash[..8.min(chunk_hash.len())],
                    shard_index,
                    &from_peer[..8.min(from_peer.len())],
                    data.len()
                );

                // Shard lokal cachen fÃ¼r zukÃ¼nftige Anfragen
                if let Err(e) = shard_store.write_shard(&chunk_hash, shard_index, &data) {
                    eprintln!("[sharding] âš  Shard lokal cachen fehlgeschlagen: {e}");
                }

                fetched.insert(shard_index, data);
            }
            Ok(Ok(crate::network::NetworkEvent::ShardRequestFailed {
                chunk_hash,
                shard_index,
                peer_id,
                error,
            })) if chunk_hash == chunk_ref.hash => {
                eprintln!(
                    "[sharding] âš  Shard {}[{}] von {} fehlgeschlagen: {error}",
                    &chunk_hash[..8.min(chunk_hash.len())],
                    shard_index,
                    &peer_id[..8.min(peer_id.len())]
                );
            }
            Ok(Ok(_)) => {
                // Anderes Event â†’ ignorieren, weiter warten
                continue;
            }
            Ok(Err(broadcast::error::RecvError::Lagged(n))) => {
                eprintln!("[sharding] Event-Buffer Ã¼bergelaufen ({n} verpasst)");
                continue;
            }
            Ok(Err(_)) => break, // Channel geschlossen
            Err(_) => break,     // Timeout
        }
    }

    println!(
        "[sharding] ğŸ“¦ {} Remote-Shard(s) fÃ¼r {} empfangen",
        fetched.len(),
        &chunk_ref.hash[..8.min(chunk_ref.hash.len())]
    );
    fetched
}

// â”€â”€â”€ Tests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#[cfg(test)]
mod tests {
    use super::*;
    use crate::blockchain::{Block, NodeRole};

    fn make_test_block(index: u64, prev_hash: &str) -> Block {
        Block {
            index,
            timestamp: 0,
            merkle_root: format!("{:x}", Sha256::digest(b"test")),
            data_size: 0,
            previous_hash: prev_hash.to_string(),
            hash: format!("hash{index:064}"),
            signer: "test".into(),
            signature: String::new(),
            owner: "test".into(),
            documents: Vec::new(),
            tombstones: Vec::new(),
            node_role: NodeRole::Master,
            proposal_round: 0,
            validator_pub_key: String::new(),
            validator_signature: String::new(),
        }
    }

    #[test]
    fn test_chunk_roundtrip() {
        let store = ChunkStore::new().unwrap();
        let data = b"Hello Stone Storage!";
        let hash = store.write_chunk(data).unwrap();
        assert_eq!(hash.len(), 64);
        let read_back = store.read_chunk(&hash).unwrap();
        assert_eq!(data.as_ref(), read_back.as_slice());
    }

    #[test]
    fn test_chunk_deduplication() {
        let store = ChunkStore::new().unwrap();
        let data = b"Deduplizierter Inhalt";
        let h1 = store.write_chunk(data).unwrap();
        let h2 = store.write_chunk(data).unwrap();
        assert_eq!(h1, h2, "Gleicher Inhalt muss gleichen Hash produzieren");
    }

    #[test]
    fn test_chunk_not_found() {
        let store = ChunkStore::new().unwrap();
        let result = store.read_chunk(&"0".repeat(64));
        assert!(result.is_err());
    }
}
